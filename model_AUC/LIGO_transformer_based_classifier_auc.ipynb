{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ba2UrRNoD_y1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 12:59:51.470753: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-20 12:59:51.470770: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/ej/workspace/hls4ml/hls4ml/hls4ml/converters/__init__.py:15: UserWarning: WARNING: Pytorch converter is not enabled!\n",
      "  warnings.warn(\"WARNING: Pytorch converter is not enabled!\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import hls4ml\n",
    "import os\n",
    "os.environ['PATH'] = '/opt/Xilinx/Vivado/2019.2/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Q7KKAw-IEZnB"
   },
   "outputs": [],
   "source": [
    "anomaly_class = {\n",
    "    'bkg': 0,\n",
    "    'glitches_new': 1\n",
    "}\n",
    "\n",
    "def readfile(anomaly_type):\n",
    "  # only look at L1 for now\n",
    "  data = np.load(anomaly_type+'.npy')[:,0,:]\n",
    "  ids = np.full(data.shape[0], anomaly_class[anomaly_type], dtype=int)  \n",
    "  return data, ids\n",
    "\n",
    "\n",
    "x = np.array([])\n",
    "y = np.array([])\n",
    "for anom in anomaly_class.keys():\n",
    "  x_anom, y_anom = readfile(anom)\n",
    "  x = np.concatenate((x, x_anom), axis=0) if x.size else x_anom\n",
    "  y = np.concatenate((y, y_anom), axis=0) if y.size else y_anom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ruL8Cd3eEZN2"
   },
   "outputs": [],
   "source": [
    "# choose the split\n",
    "split = 0.2\n",
    "\n",
    "def split_train_test(x, y, split):\n",
    "  split = int(x.shape[0]*(1-split))\n",
    "  x_train, y_train = x[:split,:], y[:split]\n",
    "  x_test, y_test = x[split:,:], y[split:]\n",
    "  x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "  x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "  return x_train, y_train, x_test, y_test\n",
    "\n",
    "np.random.seed(3)\n",
    "idx = np.random.permutation(len(x))\n",
    "x = x[idx]\n",
    "y = y[idx]\n",
    "\n",
    "\n",
    "x_train, y_train, x_test, y_test = split_train_test(x, y, split)\n",
    "\n",
    "n_classes = len(anomaly_class.values())\n",
    "\n",
    "# mix events\n",
    "idx = np.random.permutation(len(x_train))\n",
    "x_train = x_train[idx]\n",
    "y_train = y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 12:59:52.736454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-20 12:59:52.736977: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-20 12:59:52.737031: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-04-20 12:59:52.737075: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-04-20 12:59:52.737116: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-04-20 12:59:52.737158: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-04-20 12:59:52.737199: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-04-20 12:59:52.737239: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-04-20 12:59:52.737280: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-04-20 12:59:52.737287: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-04-20 12:59:52.737519: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('LIGO_transformer_based_classifier_v3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 100, 4)       8           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 100, 4)      8           ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 100, 4)      612         ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 100, 4)       0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 100, 4)       0           ['dropout[0][0]',                \n",
      "                                                                  'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 100, 4)      8           ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 100, 4)       20          ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 100, 4)       0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 100, 4)       20          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 100, 4)       0           ['dense_2[0][0]',                \n",
      "                                                                  'layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 100, 4)      8           ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 100, 4)      612         ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 100, 4)       0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 100, 4)       0           ['dropout_2[0][0]',              \n",
      "                                                                  'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 100, 4)      8           ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 100, 4)       20          ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 100, 4)       0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 100, 4)       20          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 100, 4)       0           ['dense_4[0][0]',                \n",
      "                                                                  'layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 100, 4)      8           ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 100, 1)       5           ['layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 100)          0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 20)           2020        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 20)           0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 8)            168         ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 8)            0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 2)            18          ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,563\n",
      "Trainable params: 3,563\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 5ms/step\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<8,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'dense': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'dense_linear': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'layer_normalization': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<8,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<8,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'dense_1_relu': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'dense_2': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'dense_2_linear': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'add_1': {'Precision': 'ap_fixed<8,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<8,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<8,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'dense_3_relu': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'dense_4': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'dense_4_linear': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'add_3': {'Precision': 'ap_fixed<8,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'dense_5_relu': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'dense_6': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'dense_6_relu': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'dense_7': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'dense_7_relu': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>'}, 'dense_8': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<8,6>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<8,6>', 'scale': 'ap_fixed<8,6>', 'bias': 'ap_fixed<8,6>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<10,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'dense': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'dense_linear': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'layer_normalization': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<10,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<10,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'dense_1_relu': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'dense_2': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'dense_2_linear': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'add_1': {'Precision': 'ap_fixed<10,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<10,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<10,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'dense_3_relu': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'dense_4': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'dense_4_linear': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'add_3': {'Precision': 'ap_fixed<10,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'dense_5_relu': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'dense_6': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'dense_6_relu': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'dense_7': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'dense_7_relu': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>'}, 'dense_8': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<10,6>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<10,6>', 'scale': 'ap_fixed<10,6>', 'bias': 'ap_fixed<10,6>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<12,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'dense': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'dense_linear': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'layer_normalization': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<12,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<12,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'dense_1_relu': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'dense_2': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'dense_2_linear': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'add_1': {'Precision': 'ap_fixed<12,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<12,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<12,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'dense_3_relu': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'dense_4': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'dense_4_linear': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'add_3': {'Precision': 'ap_fixed<12,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'dense_5_relu': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'dense_6': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'dense_6_relu': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'dense_7': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'dense_7_relu': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>'}, 'dense_8': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<12,6>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<12,6>', 'scale': 'ap_fixed<12,6>', 'bias': 'ap_fixed<12,6>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<14,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'dense': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'dense_linear': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'layer_normalization': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<14,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<14,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'dense_1_relu': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'dense_2': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'dense_2_linear': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'add_1': {'Precision': 'ap_fixed<14,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<14,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<14,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'dense_3_relu': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'dense_4': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'dense_4_linear': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'add_3': {'Precision': 'ap_fixed<14,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'dense_5_relu': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'dense_6': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'dense_6_relu': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'dense_7': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'dense_7_relu': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>'}, 'dense_8': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<14,6>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<14,6>', 'scale': 'ap_fixed<14,6>', 'bias': 'ap_fixed<14,6>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<16,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'dense': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'dense_linear': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'layer_normalization': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<16,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<16,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'dense_1_relu': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'dense_2': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'dense_2_linear': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'add_1': {'Precision': 'ap_fixed<16,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<16,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<16,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'dense_3_relu': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'dense_4': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'dense_4_linear': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'add_3': {'Precision': 'ap_fixed<16,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'dense_5_relu': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'dense_6': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'dense_6_relu': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'dense_7': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'dense_7_relu': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>'}, 'dense_8': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<16,6>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<16,6>', 'scale': 'ap_fixed<16,6>', 'bias': 'ap_fixed<16,6>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<18,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'dense': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'dense_linear': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'layer_normalization': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<18,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<18,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'dense_1_relu': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'dense_2': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'dense_2_linear': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'add_1': {'Precision': 'ap_fixed<18,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<18,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<18,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'dense_3_relu': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'dense_4': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'dense_4_linear': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'add_3': {'Precision': 'ap_fixed<18,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'dense_5_relu': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'dense_6': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'dense_6_relu': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'dense_7': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'dense_7_relu': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>'}, 'dense_8': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<18,6>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<18,6>', 'scale': 'ap_fixed<18,6>', 'bias': 'ap_fixed<18,6>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<20,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'dense': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'dense_linear': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'layer_normalization': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<20,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<20,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'dense_1_relu': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'dense_2': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'dense_2_linear': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'add_1': {'Precision': 'ap_fixed<20,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<20,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<20,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'dense_3_relu': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'dense_4': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'dense_4_linear': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'add_3': {'Precision': 'ap_fixed<20,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'dense_5_relu': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'dense_6': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'dense_6_relu': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'dense_7': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'dense_7_relu': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>'}, 'dense_8': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<20,6>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<20,6>', 'scale': 'ap_fixed<20,6>', 'bias': 'ap_fixed<20,6>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<22,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'dense': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'dense_linear': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'layer_normalization': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<22,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<22,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'dense_1_relu': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'dense_2': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'dense_2_linear': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'add_1': {'Precision': 'ap_fixed<22,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<22,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<22,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'dense_3_relu': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'dense_4': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'dense_4_linear': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'add_3': {'Precision': 'ap_fixed<22,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'dense_5_relu': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'dense_6': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'dense_6_relu': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'dense_7': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'dense_7_relu': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>'}, 'dense_8': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<22,6>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<22,6>', 'scale': 'ap_fixed<22,6>', 'bias': 'ap_fixed<22,6>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<24,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'dense': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'dense_linear': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'layer_normalization': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<24,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<24,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'dense_1_relu': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'dense_2': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'dense_2_linear': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'add_1': {'Precision': 'ap_fixed<24,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<24,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<24,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'dense_3_relu': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'dense_4': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'dense_4_linear': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'add_3': {'Precision': 'ap_fixed<24,6>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'dense_5_relu': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'dense_6': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'dense_6_relu': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'dense_7': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'dense_7_relu': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>'}, 'dense_8': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<24,6>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<24,6>', 'scale': 'ap_fixed<24,6>', 'bias': 'ap_fixed<24,6>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "auc_arr = np.array([])\n",
    "int_bit = 6\n",
    "x_test1=np.ascontiguousarray(x_test[100:200,:,:])\n",
    "y_keras = model.predict(x_test1)\n",
    "y_test_binary = np.where(y_keras > 0.5, 1, 0)\n",
    "for i in range(2, 19, 2):\n",
    "    precision = 'ap_fixed<{},{}>'.format((int_bit+i), int_bit)\n",
    "    dirc = 'GW_precision/{}int_{}frac'.format(int_bit, i)\n",
    "    #First, the baseline model\n",
    "    hls_config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "\n",
    "    # Set the precision and reuse factor for the full model\n",
    "    hls_config['Model']['Precision'] = precision\n",
    "    hls_config['Model']['ReuseFactor'] = 1\n",
    "    hls_config['Model']['Strategy'] = 'Resource'\n",
    "\n",
    "    for Layer in hls_config['LayerName'].keys():\n",
    "        hls_config['LayerName'][Layer]['Precision'] = precision\n",
    "        hls_config['LayerName'][Layer]['Strategy'] = 'Resource'\n",
    "        hls_config['LayerName'][Layer]['ReuseFactor'] = 1\n",
    "        hls_config['LayerName'][Layer]['weight'] = precision\n",
    "        hls_config['LayerName'][Layer]['scale'] = precision\n",
    "        hls_config['LayerName'][Layer]['bias'] = precision\n",
    "        if 'layer_norm' in Layer:\n",
    "            hls_config['LayerName'][Layer]['table_t'] = 'ap_fixed<18,8>'\n",
    "        if 'multi_head_attention' in Layer:\n",
    "            hls_config['LayerName'][Layer]['table_t'] = 'ap_fixed<18,8>'\n",
    "            hls_config['LayerName'][Layer]['inv_range'] = 256\n",
    "            hls_config['LayerName'][Layer]['exp_range'] = 4\n",
    "            hls_config['LayerName'][Layer]['table_size'] = 2048\n",
    "            hls_config['LayerName'][Layer]['accum_t'] = 'ap_fixed<22,11>'\n",
    "        if 'dense_8' in Layer:\n",
    "            hls_config['LayerName'][Layer]['table_t'] = 'ap_fixed<26,13>'\n",
    "            hls_config['LayerName'][Layer]['accum_t'] = 'ap_fixed<26,13>'\n",
    "            hls_config['LayerName'][Layer]['inv_range'] = 256\n",
    "            hls_config['LayerName'][Layer]['table_size'] = 2048\n",
    "            hls_config['LayerName'][Layer]['exp_range'] = 8\n",
    "\n",
    "    hls_config['LayerName']['layer_normalization']['table_range'] = 1\n",
    "    hls_config['LayerName']['layer_normalization_1']['table_range'] = 4\n",
    "    hls_config['LayerName']['layer_normalization_2']['table_range'] = 3\n",
    "    hls_config['LayerName']['layer_normalization_3']['table_range'] = 7\n",
    "    hls_config['LayerName']['layer_normalization_3']['table_range'] = 4\n",
    "    print(hls_config)\n",
    "    #If you want best numerical performance for high-accuray models, while the default latency strategy is faster but numerically more unstable\n",
    "    # hls_config['LayerName']['output_softmax']['Strategy'] = 'Stable'\n",
    "    # plotting.print_dict(hls_config)\n",
    "\n",
    "    cfg = hls4ml.converters.create_config(backend='Vivado')\n",
    "    cfg['IOType']     = 'io_parallel' # Must set this if using CNNs!\n",
    "    cfg['HLSConfig']  = hls_config\n",
    "    cfg['KerasModel'] = model\n",
    "    cfg['OutputDir']  = dirc\n",
    "    cfg['Part'] = 'xcvu13p-fhga2104-2L-e'\n",
    "\n",
    "    hls_model = hls4ml.converters.keras_to_hls(cfg)\n",
    "    hls_model.compile()\n",
    "    y_hls = hls_model.predict(np.ascontiguousarray(x_test1, dtype=np.float32))\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_binary.ravel(), y_hls.ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    auc_arr = np.append(auc_arr, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04    0.02065 0.0204  0.2911  0.9999  1.      0.9991  0.9947  0.9996 ]\n"
     ]
    }
   ],
   "source": [
    "print(auc_arr)\n",
    "int6 = auc_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 6ms/step\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<9,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'dense': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'dense_linear': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'layer_normalization': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<9,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<9,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'dense_1_relu': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'dense_2': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'dense_2_linear': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'add_1': {'Precision': 'ap_fixed<9,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<9,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<9,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'dense_3_relu': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'dense_4': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'dense_4_linear': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'add_3': {'Precision': 'ap_fixed<9,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'dense_5_relu': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'dense_6': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'dense_6_relu': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'dense_7': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'dense_7_relu': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>'}, 'dense_8': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<9,7>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<9,7>', 'scale': 'ap_fixed<9,7>', 'bias': 'ap_fixed<9,7>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<11,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'dense': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'dense_linear': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'layer_normalization': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<11,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<11,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'dense_1_relu': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'dense_2': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'dense_2_linear': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'add_1': {'Precision': 'ap_fixed<11,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<11,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<11,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'dense_3_relu': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'dense_4': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'dense_4_linear': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'add_3': {'Precision': 'ap_fixed<11,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'dense_5_relu': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'dense_6': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'dense_6_relu': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'dense_7': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'dense_7_relu': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>'}, 'dense_8': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<11,7>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,7>', 'scale': 'ap_fixed<11,7>', 'bias': 'ap_fixed<11,7>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<13,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'dense': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'dense_linear': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'layer_normalization': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<13,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<13,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'dense_1_relu': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'dense_2': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'dense_2_linear': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'add_1': {'Precision': 'ap_fixed<13,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<13,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<13,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'dense_3_relu': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'dense_4': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'dense_4_linear': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'add_3': {'Precision': 'ap_fixed<13,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'dense_5_relu': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'dense_6': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'dense_6_relu': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'dense_7': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'dense_7_relu': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>'}, 'dense_8': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<13,7>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,7>', 'scale': 'ap_fixed<13,7>', 'bias': 'ap_fixed<13,7>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<15,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'dense': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'dense_linear': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'layer_normalization': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<15,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<15,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'dense_1_relu': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'dense_2': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'dense_2_linear': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'add_1': {'Precision': 'ap_fixed<15,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<15,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<15,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'dense_3_relu': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'dense_4': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'dense_4_linear': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'add_3': {'Precision': 'ap_fixed<15,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'dense_5_relu': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'dense_6': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'dense_6_relu': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'dense_7': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'dense_7_relu': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>'}, 'dense_8': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<15,7>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,7>', 'scale': 'ap_fixed<15,7>', 'bias': 'ap_fixed<15,7>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<17,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'dense': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'dense_linear': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'layer_normalization': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<17,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<17,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'dense_1_relu': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'dense_2': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'dense_2_linear': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'add_1': {'Precision': 'ap_fixed<17,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<17,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<17,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'dense_3_relu': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'dense_4': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'dense_4_linear': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'add_3': {'Precision': 'ap_fixed<17,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'dense_5_relu': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'dense_6': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'dense_6_relu': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'dense_7': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'dense_7_relu': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>'}, 'dense_8': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<17,7>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,7>', 'scale': 'ap_fixed<17,7>', 'bias': 'ap_fixed<17,7>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<19,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'dense': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'dense_linear': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'layer_normalization': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<19,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<19,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'dense_1_relu': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'dense_2': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'dense_2_linear': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'add_1': {'Precision': 'ap_fixed<19,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<19,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<19,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'dense_3_relu': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'dense_4': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'dense_4_linear': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'add_3': {'Precision': 'ap_fixed<19,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'dense_5_relu': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'dense_6': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'dense_6_relu': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'dense_7': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'dense_7_relu': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>'}, 'dense_8': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<19,7>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,7>', 'scale': 'ap_fixed<19,7>', 'bias': 'ap_fixed<19,7>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<21,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'dense': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'dense_linear': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'layer_normalization': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<21,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<21,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'dense_1_relu': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'dense_2': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'dense_2_linear': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'add_1': {'Precision': 'ap_fixed<21,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<21,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<21,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'dense_3_relu': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'dense_4': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'dense_4_linear': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'add_3': {'Precision': 'ap_fixed<21,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'dense_5_relu': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'dense_6': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'dense_6_relu': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'dense_7': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'dense_7_relu': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>'}, 'dense_8': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<21,7>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,7>', 'scale': 'ap_fixed<21,7>', 'bias': 'ap_fixed<21,7>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<23,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'dense': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'dense_linear': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'layer_normalization': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<23,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<23,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'dense_1_relu': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'dense_2': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'dense_2_linear': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'add_1': {'Precision': 'ap_fixed<23,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<23,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<23,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'dense_3_relu': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'dense_4': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'dense_4_linear': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'add_3': {'Precision': 'ap_fixed<23,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'dense_5_relu': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'dense_6': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'dense_6_relu': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'dense_7': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'dense_7_relu': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>'}, 'dense_8': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<23,7>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,7>', 'scale': 'ap_fixed<23,7>', 'bias': 'ap_fixed<23,7>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<25,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'dense': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'dense_linear': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'layer_normalization': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<25,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<25,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'dense_1_relu': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'dense_2': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'dense_2_linear': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'add_1': {'Precision': 'ap_fixed<25,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<25,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<25,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'dense_3_relu': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'dense_4': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'dense_4_linear': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'add_3': {'Precision': 'ap_fixed<25,7>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'dense_5_relu': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'dense_6': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'dense_6_relu': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'dense_7': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'dense_7_relu': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>'}, 'dense_8': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<25,7>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,7>', 'scale': 'ap_fixed<25,7>', 'bias': 'ap_fixed<25,7>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "auc_arr = np.array([])\n",
    "int_bit = 7\n",
    "x_test1=np.ascontiguousarray(x_test[100:200,:,:])\n",
    "y_keras = model.predict(x_test1)\n",
    "y_test_binary = np.where(y_keras > 0.5, 1, 0)\n",
    "for i in range(2, 19, 2):\n",
    "    precision = 'ap_fixed<{},{}>'.format((int_bit+i), int_bit)\n",
    "    dirc = 'GW_precision/{}int_{}frac'.format(int_bit, i)\n",
    "    #First, the baseline model\n",
    "    hls_config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "\n",
    "    # Set the precision and reuse factor for the full model\n",
    "    hls_config['Model']['Precision'] = precision\n",
    "    hls_config['Model']['ReuseFactor'] = 1\n",
    "    hls_config['Model']['Strategy'] = 'Resource'\n",
    "\n",
    "    for Layer in hls_config['LayerName'].keys():\n",
    "        hls_config['LayerName'][Layer]['Precision'] = precision\n",
    "        hls_config['LayerName'][Layer]['Strategy'] = 'Resource'\n",
    "        hls_config['LayerName'][Layer]['ReuseFactor'] = 1\n",
    "        hls_config['LayerName'][Layer]['weight'] = precision\n",
    "        hls_config['LayerName'][Layer]['scale'] = precision\n",
    "        hls_config['LayerName'][Layer]['bias'] = precision\n",
    "        if 'layer_norm' in Layer:\n",
    "            hls_config['LayerName'][Layer]['table_t'] = 'ap_fixed<18,8>'\n",
    "        if 'multi_head_attention' in Layer:\n",
    "            hls_config['LayerName'][Layer]['table_t'] = 'ap_fixed<18,8>'\n",
    "            hls_config['LayerName'][Layer]['inv_range'] = 256\n",
    "            hls_config['LayerName'][Layer]['exp_range'] = 4\n",
    "            hls_config['LayerName'][Layer]['table_size'] = 2048\n",
    "            hls_config['LayerName'][Layer]['accum_t'] = 'ap_fixed<22,11>'\n",
    "        if 'dense_8' in Layer:\n",
    "            hls_config['LayerName'][Layer]['table_t'] = 'ap_fixed<26,13>'\n",
    "            hls_config['LayerName'][Layer]['accum_t'] = 'ap_fixed<26,13>'\n",
    "            hls_config['LayerName'][Layer]['inv_range'] = 256\n",
    "            hls_config['LayerName'][Layer]['table_size'] = 2048\n",
    "            hls_config['LayerName'][Layer]['exp_range'] = 8\n",
    "\n",
    "    hls_config['LayerName']['layer_normalization']['table_range'] = 1\n",
    "    hls_config['LayerName']['layer_normalization_1']['table_range'] = 4\n",
    "    hls_config['LayerName']['layer_normalization_2']['table_range'] = 3\n",
    "    hls_config['LayerName']['layer_normalization_3']['table_range'] = 7\n",
    "    hls_config['LayerName']['layer_normalization_3']['table_range'] = 4\n",
    "    print(hls_config)\n",
    "    #If you want best numerical performance for high-accuray models, while the default latency strategy is faster but numerically more unstable\n",
    "    # hls_config['LayerName']['output_softmax']['Strategy'] = 'Stable'\n",
    "    # plotting.print_dict(hls_config)\n",
    "\n",
    "    cfg = hls4ml.converters.create_config(backend='Vivado')\n",
    "    cfg['IOType']     = 'io_parallel' # Must set this if using CNNs!\n",
    "    cfg['HLSConfig']  = hls_config\n",
    "    cfg['KerasModel'] = model\n",
    "    cfg['OutputDir']  = dirc\n",
    "    cfg['Part'] = 'xcvu13p-fhga2104-2L-e'\n",
    "\n",
    "    hls_model = hls4ml.converters.keras_to_hls(cfg)\n",
    "    hls_model.compile()\n",
    "    y_hls = hls_model.predict(np.ascontiguousarray(x_test1, dtype=np.float32))\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_binary.ravel(), y_hls.ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    auc_arr = np.append(auc_arr, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04    0.0303  0.04    0.3101  0.98935 0.9999  0.9999  0.992   1.     ]\n"
     ]
    }
   ],
   "source": [
    "print(auc_arr)\n",
    "int7 = auc_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 6ms/step\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<10,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'dense': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'dense_linear': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'layer_normalization': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<10,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<10,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'dense_1_relu': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'dense_2': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'dense_2_linear': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'add_1': {'Precision': 'ap_fixed<10,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<10,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<10,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'dense_3_relu': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'dense_4': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'dense_4_linear': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'add_3': {'Precision': 'ap_fixed<10,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'dense_5_relu': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'dense_6': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'dense_6_relu': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'dense_7': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'dense_7_relu': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>'}, 'dense_8': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<10,8>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<10,8>', 'scale': 'ap_fixed<10,8>', 'bias': 'ap_fixed<10,8>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<12,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'dense': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'dense_linear': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'layer_normalization': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<12,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<12,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'dense_1_relu': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'dense_2': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'dense_2_linear': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'add_1': {'Precision': 'ap_fixed<12,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<12,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<12,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'dense_3_relu': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'dense_4': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'dense_4_linear': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'add_3': {'Precision': 'ap_fixed<12,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'dense_5_relu': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'dense_6': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'dense_6_relu': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'dense_7': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'dense_7_relu': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>'}, 'dense_8': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<12,8>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<12,8>', 'scale': 'ap_fixed<12,8>', 'bias': 'ap_fixed<12,8>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<14,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'dense': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'dense_linear': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'layer_normalization': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<14,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<14,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'dense_1_relu': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'dense_2': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'dense_2_linear': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'add_1': {'Precision': 'ap_fixed<14,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<14,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<14,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'dense_3_relu': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'dense_4': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'dense_4_linear': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'add_3': {'Precision': 'ap_fixed<14,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'dense_5_relu': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'dense_6': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'dense_6_relu': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'dense_7': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'dense_7_relu': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>'}, 'dense_8': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<14,8>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<14,8>', 'scale': 'ap_fixed<14,8>', 'bias': 'ap_fixed<14,8>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<16,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'dense': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'dense_linear': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'layer_normalization': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<16,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<16,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'dense_1_relu': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'dense_2': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'dense_2_linear': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'add_1': {'Precision': 'ap_fixed<16,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<16,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<16,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'dense_3_relu': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'dense_4': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'dense_4_linear': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'add_3': {'Precision': 'ap_fixed<16,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'dense_5_relu': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'dense_6': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'dense_6_relu': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'dense_7': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'dense_7_relu': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>'}, 'dense_8': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<16,8>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<16,8>', 'scale': 'ap_fixed<16,8>', 'bias': 'ap_fixed<16,8>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'dense': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'dense_linear': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'layer_normalization': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'dense_1_relu': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'dense_2': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'dense_2_linear': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'add_1': {'Precision': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'dense_3_relu': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'dense_4': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'dense_4_linear': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'add_3': {'Precision': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'dense_5_relu': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'dense_6': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'dense_6_relu': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'dense_7': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'dense_7_relu': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>'}, 'dense_8': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<18,8>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<18,8>', 'scale': 'ap_fixed<18,8>', 'bias': 'ap_fixed<18,8>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<20,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'dense': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'dense_linear': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'layer_normalization': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<20,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<20,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'dense_1_relu': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'dense_2': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'dense_2_linear': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'add_1': {'Precision': 'ap_fixed<20,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<20,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<20,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'dense_3_relu': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'dense_4': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'dense_4_linear': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'add_3': {'Precision': 'ap_fixed<20,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'dense_5_relu': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'dense_6': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'dense_6_relu': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'dense_7': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'dense_7_relu': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>'}, 'dense_8': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<20,8>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<20,8>', 'scale': 'ap_fixed<20,8>', 'bias': 'ap_fixed<20,8>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<22,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'dense': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'dense_linear': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'layer_normalization': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<22,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<22,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'dense_1_relu': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'dense_2': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'dense_2_linear': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'add_1': {'Precision': 'ap_fixed<22,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<22,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<22,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'dense_3_relu': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'dense_4': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'dense_4_linear': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'add_3': {'Precision': 'ap_fixed<22,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'dense_5_relu': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'dense_6': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'dense_6_relu': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'dense_7': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'dense_7_relu': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>'}, 'dense_8': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<22,8>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<22,8>', 'scale': 'ap_fixed<22,8>', 'bias': 'ap_fixed<22,8>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<24,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'dense': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'dense_linear': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'layer_normalization': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<24,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<24,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'dense_1_relu': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'dense_2': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'dense_2_linear': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'add_1': {'Precision': 'ap_fixed<24,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<24,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<24,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'dense_3_relu': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'dense_4': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'dense_4_linear': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'add_3': {'Precision': 'ap_fixed<24,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'dense_5_relu': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'dense_6': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'dense_6_relu': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'dense_7': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'dense_7_relu': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>'}, 'dense_8': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<24,8>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<24,8>', 'scale': 'ap_fixed<24,8>', 'bias': 'ap_fixed<24,8>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<26,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'dense': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'dense_linear': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'layer_normalization': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<26,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<26,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'dense_1_relu': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'dense_2': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'dense_2_linear': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'add_1': {'Precision': 'ap_fixed<26,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<26,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<26,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'dense_3_relu': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'dense_4': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'dense_4_linear': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'add_3': {'Precision': 'ap_fixed<26,8>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'dense_5_relu': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'dense_6': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'dense_6_relu': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'dense_7': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'dense_7_relu': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>'}, 'dense_8': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<26,8>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<26,8>', 'scale': 'ap_fixed<26,8>', 'bias': 'ap_fixed<26,8>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "auc_arr = np.array([])\n",
    "int_bit = 8\n",
    "x_test1=np.ascontiguousarray(x_test[100:200,:,:])\n",
    "y_keras = model.predict(x_test1)\n",
    "y_test_binary = np.where(y_keras > 0.5, 1, 0)\n",
    "for i in range(2, 19, 2):\n",
    "    precision = 'ap_fixed<{},{}>'.format((int_bit+i), int_bit)\n",
    "    dirc = 'GW_precision/{}int_{}frac'.format(int_bit, i)\n",
    "    #First, the baseline model\n",
    "    hls_config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "\n",
    "    # Set the precision and reuse factor for the full model\n",
    "    hls_config['Model']['Precision'] = precision\n",
    "    hls_config['Model']['ReuseFactor'] = 1\n",
    "    hls_config['Model']['Strategy'] = 'Resource'\n",
    "\n",
    "    for Layer in hls_config['LayerName'].keys():\n",
    "        hls_config['LayerName'][Layer]['Precision'] = precision\n",
    "        hls_config['LayerName'][Layer]['Strategy'] = 'Resource'\n",
    "        hls_config['LayerName'][Layer]['ReuseFactor'] = 1\n",
    "        hls_config['LayerName'][Layer]['weight'] = precision\n",
    "        hls_config['LayerName'][Layer]['scale'] = precision\n",
    "        hls_config['LayerName'][Layer]['bias'] = precision\n",
    "        if 'layer_norm' in Layer:\n",
    "            hls_config['LayerName'][Layer]['table_t'] = 'ap_fixed<18,8>'\n",
    "        if 'multi_head_attention' in Layer:\n",
    "            hls_config['LayerName'][Layer]['table_t'] = 'ap_fixed<18,8>'\n",
    "            hls_config['LayerName'][Layer]['inv_range'] = 256\n",
    "            hls_config['LayerName'][Layer]['exp_range'] = 4\n",
    "            hls_config['LayerName'][Layer]['table_size'] = 2048\n",
    "            hls_config['LayerName'][Layer]['accum_t'] = 'ap_fixed<22,11>'\n",
    "        if 'dense_8' in Layer:\n",
    "            hls_config['LayerName'][Layer]['table_t'] = 'ap_fixed<26,13>'\n",
    "            hls_config['LayerName'][Layer]['accum_t'] = 'ap_fixed<26,13>'\n",
    "            hls_config['LayerName'][Layer]['inv_range'] = 256\n",
    "            hls_config['LayerName'][Layer]['table_size'] = 2048\n",
    "            hls_config['LayerName'][Layer]['exp_range'] = 8\n",
    "\n",
    "    hls_config['LayerName']['layer_normalization']['table_range'] = 1\n",
    "    hls_config['LayerName']['layer_normalization_1']['table_range'] = 4\n",
    "    hls_config['LayerName']['layer_normalization_2']['table_range'] = 3\n",
    "    hls_config['LayerName']['layer_normalization_3']['table_range'] = 7\n",
    "    hls_config['LayerName']['layer_normalization_3']['table_range'] = 4\n",
    "    print(hls_config)\n",
    "    #If you want best numerical performance for high-accuray models, while the default latency strategy is faster but numerically more unstable\n",
    "    # hls_config['LayerName']['output_softmax']['Strategy'] = 'Stable'\n",
    "    # plotting.print_dict(hls_config)\n",
    "\n",
    "    cfg = hls4ml.converters.create_config(backend='Vivado')\n",
    "    cfg['IOType']     = 'io_parallel' # Must set this if using CNNs!\n",
    "    cfg['HLSConfig']  = hls_config\n",
    "    cfg['KerasModel'] = model\n",
    "    cfg['OutputDir']  = dirc\n",
    "    cfg['Part'] = 'xcvu13p-fhga2104-2L-e'\n",
    "\n",
    "    hls_model = hls4ml.converters.keras_to_hls(cfg)\n",
    "    hls_model.compile()\n",
    "    y_hls = hls_model.predict(np.ascontiguousarray(x_test1, dtype=np.float32))\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_binary.ravel(), y_hls.ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    auc_arr = np.append(auc_arr, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04    0.04    0.04    0.29385 0.9999  1.      1.      0.9921  1.     ]\n"
     ]
    }
   ],
   "source": [
    "print(auc_arr)\n",
    "int8 = auc_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 6ms/step\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<11,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_linear': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'layer_normalization': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<11,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<11,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_1_relu': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_2': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_2_linear': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'add_1': {'Precision': 'ap_fixed<11,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<11,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<11,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_3_relu': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_4': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_4_linear': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'add_3': {'Precision': 'ap_fixed<11,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_5_relu': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_6': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_6_relu': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_7': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_7_relu': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_8': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<13,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_linear': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'layer_normalization': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<13,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<13,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_1_relu': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_2': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_2_linear': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'add_1': {'Precision': 'ap_fixed<13,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<13,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<13,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_3_relu': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_4': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_4_linear': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'add_3': {'Precision': 'ap_fixed<13,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_5_relu': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_6': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_6_relu': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_7': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_7_relu': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_8': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<15,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_linear': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'layer_normalization': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<15,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<15,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_1_relu': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_2': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_2_linear': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'add_1': {'Precision': 'ap_fixed<15,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<15,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<15,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_3_relu': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_4': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_4_linear': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'add_3': {'Precision': 'ap_fixed<15,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_5_relu': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_6': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_6_relu': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_7': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_7_relu': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_8': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<17,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_linear': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'layer_normalization': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<17,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<17,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_1_relu': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_2': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_2_linear': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'add_1': {'Precision': 'ap_fixed<17,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<17,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<17,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_3_relu': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_4': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_4_linear': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'add_3': {'Precision': 'ap_fixed<17,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_5_relu': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_6': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_6_relu': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_7': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_7_relu': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_8': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<19,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_linear': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'layer_normalization': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<19,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<19,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_1_relu': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_2': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_2_linear': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'add_1': {'Precision': 'ap_fixed<19,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<19,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<19,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_3_relu': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_4': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_4_linear': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'add_3': {'Precision': 'ap_fixed<19,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_5_relu': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_6': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_6_relu': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_7': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_7_relu': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_8': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<21,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_linear': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'layer_normalization': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<21,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<21,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_1_relu': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_2': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_2_linear': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'add_1': {'Precision': 'ap_fixed<21,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<21,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<21,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_3_relu': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_4': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_4_linear': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'add_3': {'Precision': 'ap_fixed<21,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_5_relu': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_6': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_6_relu': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_7': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_7_relu': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_8': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<23,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_linear': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'layer_normalization': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<23,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<23,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_1_relu': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_2': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_2_linear': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'add_1': {'Precision': 'ap_fixed<23,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<23,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<23,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_3_relu': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_4': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_4_linear': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'add_3': {'Precision': 'ap_fixed<23,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_5_relu': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_6': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_6_relu': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_7': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_7_relu': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_8': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<25,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_linear': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'layer_normalization': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<25,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<25,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_1_relu': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_2': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_2_linear': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'add_1': {'Precision': 'ap_fixed<25,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<25,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<25,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_3_relu': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_4': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_4_linear': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'add_3': {'Precision': 'ap_fixed<25,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_5_relu': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_6': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_6_relu': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_7': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_7_relu': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_8': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<27,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_linear': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'layer_normalization': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<27,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<27,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_1_relu': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_2': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_2_linear': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'add_1': {'Precision': 'ap_fixed<27,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<27,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<27,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_3_relu': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_4': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_4_linear': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'add_3': {'Precision': 'ap_fixed<27,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_5_relu': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_6': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_6_relu': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_7': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_7_relu': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_8': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "auc_arr = np.array([])\n",
    "int_bit = 9\n",
    "x_test1=np.ascontiguousarray(x_test[100:200,:,:])\n",
    "y_keras = model.predict(x_test1)\n",
    "y_test_binary = np.where(y_keras > 0.5, 1, 0)\n",
    "for i in range(2, 19, 2):\n",
    "    precision = 'ap_fixed<{},{}>'.format((int_bit+i), int_bit)\n",
    "    dirc = 'GW_precision/{}int_{}frac'.format(int_bit, i)\n",
    "    #First, the baseline model\n",
    "    hls_config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "\n",
    "    # Set the precision and reuse factor for the full model\n",
    "    hls_config['Model']['Precision'] = precision\n",
    "    hls_config['Model']['ReuseFactor'] = 1\n",
    "    hls_config['Model']['Strategy'] = 'Resource'\n",
    "\n",
    "    for Layer in hls_config['LayerName'].keys():\n",
    "        hls_config['LayerName'][Layer]['Precision'] = precision\n",
    "        hls_config['LayerName'][Layer]['Strategy'] = 'Resource'\n",
    "        hls_config['LayerName'][Layer]['ReuseFactor'] = 1\n",
    "        hls_config['LayerName'][Layer]['weight'] = precision\n",
    "        hls_config['LayerName'][Layer]['scale'] = precision\n",
    "        hls_config['LayerName'][Layer]['bias'] = precision\n",
    "        if 'layer_norm' in Layer:\n",
    "            hls_config['LayerName'][Layer]['table_t'] = 'ap_fixed<18,8>'\n",
    "        if 'multi_head_attention' in Layer:\n",
    "            hls_config['LayerName'][Layer]['table_t'] = 'ap_fixed<18,8>'\n",
    "            hls_config['LayerName'][Layer]['inv_range'] = 256\n",
    "            hls_config['LayerName'][Layer]['exp_range'] = 4\n",
    "            hls_config['LayerName'][Layer]['table_size'] = 2048\n",
    "            hls_config['LayerName'][Layer]['accum_t'] = 'ap_fixed<22,11>'\n",
    "        if 'dense_8' in Layer:\n",
    "            hls_config['LayerName'][Layer]['table_t'] = 'ap_fixed<26,13>'\n",
    "            hls_config['LayerName'][Layer]['accum_t'] = 'ap_fixed<26,13>'\n",
    "            hls_config['LayerName'][Layer]['inv_range'] = 256\n",
    "            hls_config['LayerName'][Layer]['table_size'] = 2048\n",
    "            hls_config['LayerName'][Layer]['exp_range'] = 8\n",
    "\n",
    "    hls_config['LayerName']['layer_normalization']['table_range'] = 1\n",
    "    hls_config['LayerName']['layer_normalization_1']['table_range'] = 4\n",
    "    hls_config['LayerName']['layer_normalization_2']['table_range'] = 3\n",
    "    hls_config['LayerName']['layer_normalization_3']['table_range'] = 7\n",
    "    hls_config['LayerName']['layer_normalization_3']['table_range'] = 4\n",
    "    print(hls_config)\n",
    "    #If you want best numerical performance for high-accuray models, while the default latency strategy is faster but numerically more unstable\n",
    "    # hls_config['LayerName']['output_softmax']['Strategy'] = 'Stable'\n",
    "    # plotting.print_dict(hls_config)\n",
    "\n",
    "    cfg = hls4ml.converters.create_config(backend='Vivado')\n",
    "    cfg['IOType']     = 'io_parallel' # Must set this if using CNNs!\n",
    "    cfg['HLSConfig']  = hls_config\n",
    "    cfg['KerasModel'] = model\n",
    "    cfg['OutputDir']  = dirc\n",
    "    cfg['Part'] = 'xcvu13p-fhga2104-2L-e'\n",
    "\n",
    "    hls_model = hls4ml.converters.keras_to_hls(cfg)\n",
    "    hls_model.compile()\n",
    "    y_hls = hls_model.predict(np.ascontiguousarray(x_test1, dtype=np.float32))\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_binary.ravel(), y_hls.ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    auc_arr = np.append(auc_arr, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04    0.02015 0.04    0.2963  1.      1.      1.      1.      1.     ]\n"
     ]
    }
   ],
   "source": [
    "print(auc_arr)\n",
    "int9 = auc_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 6ms/step\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<11,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_linear': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'layer_normalization': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<11,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<11,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_1_relu': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_2': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_2_linear': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'add_1': {'Precision': 'ap_fixed<11,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<11,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<11,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_3_relu': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_4': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_4_linear': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'add_3': {'Precision': 'ap_fixed<11,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_5_relu': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_6': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_6_relu': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_7': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_7_relu': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>'}, 'dense_8': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<11,9>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<11,9>', 'scale': 'ap_fixed<11,9>', 'bias': 'ap_fixed<11,9>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<13,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_linear': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'layer_normalization': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<13,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<13,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_1_relu': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_2': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_2_linear': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'add_1': {'Precision': 'ap_fixed<13,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<13,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<13,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_3_relu': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_4': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_4_linear': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'add_3': {'Precision': 'ap_fixed<13,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_5_relu': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_6': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_6_relu': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_7': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_7_relu': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>'}, 'dense_8': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<13,9>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<13,9>', 'scale': 'ap_fixed<13,9>', 'bias': 'ap_fixed<13,9>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<15,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_linear': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'layer_normalization': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<15,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<15,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_1_relu': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_2': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_2_linear': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'add_1': {'Precision': 'ap_fixed<15,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<15,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<15,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_3_relu': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_4': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_4_linear': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'add_3': {'Precision': 'ap_fixed<15,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_5_relu': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_6': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_6_relu': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_7': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_7_relu': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>'}, 'dense_8': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<15,9>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<15,9>', 'scale': 'ap_fixed<15,9>', 'bias': 'ap_fixed<15,9>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<17,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_linear': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'layer_normalization': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<17,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<17,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_1_relu': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_2': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_2_linear': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'add_1': {'Precision': 'ap_fixed<17,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<17,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<17,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_3_relu': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_4': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_4_linear': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'add_3': {'Precision': 'ap_fixed<17,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_5_relu': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_6': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_6_relu': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_7': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_7_relu': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>'}, 'dense_8': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<17,9>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<17,9>', 'scale': 'ap_fixed<17,9>', 'bias': 'ap_fixed<17,9>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<19,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_linear': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'layer_normalization': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<19,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<19,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_1_relu': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_2': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_2_linear': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'add_1': {'Precision': 'ap_fixed<19,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<19,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<19,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_3_relu': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_4': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_4_linear': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'add_3': {'Precision': 'ap_fixed<19,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_5_relu': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_6': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_6_relu': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_7': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_7_relu': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>'}, 'dense_8': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<19,9>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<19,9>', 'scale': 'ap_fixed<19,9>', 'bias': 'ap_fixed<19,9>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<21,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_linear': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'layer_normalization': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<21,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<21,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_1_relu': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_2': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_2_linear': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'add_1': {'Precision': 'ap_fixed<21,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<21,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<21,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_3_relu': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_4': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_4_linear': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'add_3': {'Precision': 'ap_fixed<21,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_5_relu': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_6': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_6_relu': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_7': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_7_relu': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>'}, 'dense_8': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<21,9>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<21,9>', 'scale': 'ap_fixed<21,9>', 'bias': 'ap_fixed<21,9>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<23,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_linear': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'layer_normalization': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<23,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<23,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_1_relu': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_2': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_2_linear': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'add_1': {'Precision': 'ap_fixed<23,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<23,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<23,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_3_relu': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_4': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_4_linear': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'add_3': {'Precision': 'ap_fixed<23,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_5_relu': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_6': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_6_relu': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_7': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_7_relu': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>'}, 'dense_8': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<23,9>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<23,9>', 'scale': 'ap_fixed<23,9>', 'bias': 'ap_fixed<23,9>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<25,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_linear': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'layer_normalization': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<25,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<25,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_1_relu': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_2': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_2_linear': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'add_1': {'Precision': 'ap_fixed<25,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<25,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<25,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_3_relu': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_4': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_4_linear': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'add_3': {'Precision': 'ap_fixed<25,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_5_relu': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_6': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_6_relu': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_7': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_7_relu': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>'}, 'dense_8': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<25,9>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<25,9>', 'scale': 'ap_fixed<25,9>', 'bias': 'ap_fixed<25,9>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: dense, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense\n",
      "Layer name: layer_normalization, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention\n",
      "Layer name: add, layer type: Add\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization\n",
      "Layer name: dense_1, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_1\n",
      "Layer name: dense_2, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_2\n",
      "Layer name: add_1, layer type: Add\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention\n",
      "Layer name: add_2, layer type: Add\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization\n",
      "Layer name: dense_3, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_3\n",
      "Layer name: dense_4, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense_4\n",
      "Layer name: add_3, layer type: Add\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization\n",
      "Layer name: dense_5, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_5\n",
      "Layer name: dense_6, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_6\n",
      "Layer name: dense_7, layer type: Dense\n",
      "  -> Activation (relu), layer name: dense_7\n",
      "Layer name: dense_8, layer type: Dense\n",
      "  -> Activation (softmax), layer name: dense_8\n",
      "{'Model': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource'}, 'LayerName': {'input_1': {'Precision': 'ap_fixed<27,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_linear': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'layer_normalization': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 1}, 'multi_head_attention': {'Precision': 'ap_fixed<27,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add': {'Precision': 'ap_fixed<27,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'layer_normalization_1': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_1': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_1_relu': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_2': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_2_linear': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'add_1': {'Precision': 'ap_fixed<27,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'layer_normalization_2': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 3}, 'multi_head_attention_1': {'Precision': 'ap_fixed<27,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>', 'table_t': 'ap_fixed<18,8>', 'inv_range': 256, 'exp_range': 4, 'table_size': 2048, 'accum_t': 'ap_fixed<22,11>'}, 'add_2': {'Precision': 'ap_fixed<27,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'layer_normalization_3': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>', 'table_t': 'ap_fixed<18,8>', 'table_range': 4}, 'dense_3': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_3_relu': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_4': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_4_linear': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'add_3': {'Precision': 'ap_fixed<27,9>', 'Strategy': 'Resource', 'ReuseFactor': 1, 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'layer_normalization_4': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>', 'table_t': 'ap_fixed<18,8>'}, 'dense_5': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_5_relu': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_6': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_6_relu': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_7': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_7_relu': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'table_size': 1024, 'table_t': 'ap_fixed<18,8>', 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>'}, 'dense_8': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>', 'table_t': 'ap_fixed<26,13>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'table_size': 2048, 'exp_range': 8}, 'dense_8_softmax': {'Precision': 'ap_fixed<27,9>', 'ReuseFactor': 1, 'table_size': 2048, 'table_t': 'ap_fixed<26,13>', 'Strategy': 'Resource', 'weight': 'ap_fixed<27,9>', 'scale': 'ap_fixed<27,9>', 'bias': 'ap_fixed<27,9>', 'accum_t': 'ap_fixed<26,13>', 'inv_range': 256, 'exp_range': 8}}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 100, 1]], output shape: [None, 100, 1]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 100, 1]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_1, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_2, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_1, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_2, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: multi_head_attention_1, layer type: MultiHeadAttention, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_2, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_3, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: add_3, layer type: Merge, input shapes: [[None, 100, 4], [None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: layer_normalization_4, layer type: LayerNormalization, input shapes: [[None, 100, 4]], output shape: [None, 100, 4]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 100, 4]], output shape: [None, 100, 1]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 100, 1]], output shape: [None, 100]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"multi_head_attention_1\" (MultiHeadAttention)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8\" (Dense)\n",
      "WARNING: Config parameter \"accum_t\" overwrites an existing attribute in layer \"dense_8_softmax\" (Softmax)\n",
      "not transpose\n",
      "not transpose\n",
      "Writing HLS project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "auc_arr = np.array([])\n",
    "int_bit = 9\n",
    "x_test1=np.ascontiguousarray(x_test[100:200,:,:])\n",
    "y_keras = model.predict(x_test1)\n",
    "y_test_binary = np.where(y_keras > 0.5, 1, 0)\n",
    "for i in range(2, 19, 2):\n",
    "    precision = 'ap_fixed<{},{}>'.format((int_bit+i), int_bit)\n",
    "    dirc = 'GW_precision/{}int_{}frac'.format(int_bit, i)\n",
    "    #First, the baseline model\n",
    "    hls_config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "\n",
    "    # Set the precision and reuse factor for the full model\n",
    "    hls_config['Model']['Precision'] = precision\n",
    "    hls_config['Model']['ReuseFactor'] = 1\n",
    "    hls_config['Model']['Strategy'] = 'Resource'\n",
    "\n",
    "    for Layer in hls_config['LayerName'].keys():\n",
    "        hls_config['LayerName'][Layer]['Precision'] = precision\n",
    "        hls_config['LayerName'][Layer]['Strategy'] = 'Resource'\n",
    "        hls_config['LayerName'][Layer]['ReuseFactor'] = 1\n",
    "        hls_config['LayerName'][Layer]['weight'] = precision\n",
    "        hls_config['LayerName'][Layer]['scale'] = precision\n",
    "        hls_config['LayerName'][Layer]['bias'] = precision\n",
    "        if 'layer_norm' in Layer:\n",
    "            hls_config['LayerName'][Layer]['table_t'] = 'ap_fixed<18,8>'\n",
    "        if 'multi_head_attention' in Layer:\n",
    "            hls_config['LayerName'][Layer]['table_t'] = 'ap_fixed<18,8>'\n",
    "            hls_config['LayerName'][Layer]['inv_range'] = 256\n",
    "            hls_config['LayerName'][Layer]['exp_range'] = 4\n",
    "            hls_config['LayerName'][Layer]['table_size'] = 2048\n",
    "            hls_config['LayerName'][Layer]['accum_t'] = 'ap_fixed<22,11>'\n",
    "        if 'dense_8' in Layer:\n",
    "            hls_config['LayerName'][Layer]['table_t'] = 'ap_fixed<26,13>'\n",
    "            hls_config['LayerName'][Layer]['accum_t'] = 'ap_fixed<26,13>'\n",
    "            hls_config['LayerName'][Layer]['inv_range'] = 256\n",
    "            hls_config['LayerName'][Layer]['table_size'] = 2048\n",
    "            hls_config['LayerName'][Layer]['exp_range'] = 8\n",
    "\n",
    "    hls_config['LayerName']['layer_normalization']['table_range'] = 1\n",
    "    hls_config['LayerName']['layer_normalization_1']['table_range'] = 4\n",
    "    hls_config['LayerName']['layer_normalization_2']['table_range'] = 3\n",
    "    hls_config['LayerName']['layer_normalization_3']['table_range'] = 7\n",
    "    hls_config['LayerName']['layer_normalization_3']['table_range'] = 4\n",
    "    print(hls_config)\n",
    "    #If you want best numerical performance for high-accuray models, while the default latency strategy is faster but numerically more unstable\n",
    "    # hls_config['LayerName']['output_softmax']['Strategy'] = 'Stable'\n",
    "    # plotting.print_dict(hls_config)\n",
    "\n",
    "    cfg = hls4ml.converters.create_config(backend='Vivado')\n",
    "    cfg['IOType']     = 'io_parallel' # Must set this if using CNNs!\n",
    "    cfg['HLSConfig']  = hls_config\n",
    "    cfg['KerasModel'] = model\n",
    "    cfg['OutputDir']  = dirc\n",
    "    cfg['Part'] = 'xcvu13p-fhga2104-2L-e'\n",
    "\n",
    "    hls_model = hls4ml.converters.keras_to_hls(cfg)\n",
    "    hls_model.compile()\n",
    "    y_hls = hls_model.predict(np.ascontiguousarray(x_test1, dtype=np.float32))\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_binary.ravel(), y_hls.ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    auc_arr = np.append(auc_arr, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04    0.02015 0.04    0.2963  1.      1.      1.      1.      1.     ]\n"
     ]
    }
   ],
   "source": [
    "print(auc_arr)\n",
    "int10 = auc_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x_axis = [2,4,6,8,10,12,14,16,18]\n",
    "ax.plot(x_axis, int6, \"-o\", label = \"6 int\")\n",
    "ax.plot(x_axis, int7, \"-o\", label = '7 int')\n",
    "ax.plot(x_axis, int8, \"-o\", label = '8 int')\n",
    "ax.plot(x_axis, int9, \"-o\", label = '9 int')\n",
    "ax.plot(x_axis, int10, \"-o\", label = '10 int')\n",
    "\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xlabel(\"fractional bits\")\n",
    "ax.set_ylabel(\"AUC\")\n",
    "\n",
    "# Set the title and adjust its position\n",
    "ax.set_title(\"Gravity Wave Anormaly Tagging\", loc='left', fontweight='bold', pad=20)\n",
    "\n",
    "ax.grid()\n",
    "ax.set_ylim([0.0, 1.03])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "transformer_gpu",
   "language": "python",
   "name": "transformer_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
