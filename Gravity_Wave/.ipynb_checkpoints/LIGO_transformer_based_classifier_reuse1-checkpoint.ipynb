{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVPrRfYIEQXB"
   },
   "source": [
    "Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ba2UrRNoD_y1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yrU3xOtEX-i"
   },
   "source": [
    "Load files; for now only look at L1 data, though H1 is also available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7KKAw-IEZnB"
   },
   "outputs": [],
   "source": [
    "anomaly_class = {\n",
    "    'bkg': 0,\n",
    "    'glitches_new': 1\n",
    "}\n",
    "\n",
    "def readfile(anomaly_type):\n",
    "  # only look at L1 for now\n",
    "  data = np.load(anomaly_type+'.npy')[:,0,:]\n",
    "  ids = np.full(data.shape[0], anomaly_class[anomaly_type], dtype=int)  \n",
    "  return data, ids\n",
    "\n",
    "\n",
    "x = np.array([])\n",
    "y = np.array([])\n",
    "for anom in anomaly_class.keys():\n",
    "  x_anom, y_anom = readfile(anom)\n",
    "  x = np.concatenate((x, x_anom), axis=0) if x.size else x_anom\n",
    "  y = np.concatenate((y, y_anom), axis=0) if y.size else y_anom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZqstEdAEd8T"
   },
   "source": [
    "# Create the train/test split\n",
    "Mix different event types together before the split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruL8Cd3eEZN2"
   },
   "outputs": [],
   "source": [
    "# choose the split\n",
    "split = 0.2\n",
    "\n",
    "\n",
    "\n",
    "def split_train_test(x, y, split):\n",
    "  split = int(x.shape[0]*(1-split))\n",
    "  x_train, y_train = x[:split,:], y[:split]\n",
    "  x_test, y_test = x[split:,:], y[split:]\n",
    "  x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "  x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "  return x_train, y_train, x_test, y_test\n",
    "\n",
    "np.random.seed(3)\n",
    "idx = np.random.permutation(len(x))\n",
    "x = x[idx]\n",
    "y = y[idx]\n",
    "\n",
    "\n",
    "x_train, y_train, x_test, y_test = split_train_test(x, y, split)\n",
    "\n",
    "n_classes = len(anomaly_class.values())\n",
    "\n",
    "# mix events\n",
    "idx = np.random.permutation(len(x_train))\n",
    "x_train = x_train[idx]\n",
    "y_train = y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v0I4fXnUGqEJ",
    "outputId": "7ec4a9e2-1578-4832-f048-124c5e43f1ab"
   },
   "outputs": [],
   "source": [
    "print(f'x train/test shapes: {x_train.shape} {x_test.shape}')\n",
    "print(f'y train/test shapes: {y_train.shape} {y_test.shape}')\n",
    "print(f'Number of classes: {n_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mf1_rMQQEjYP"
   },
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = layers.Add()([x, inputs])\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.Dense(ff_dim, activation=\"relu\")(res)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(inputs.shape[-1])(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = layers.Add()([x, res])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwvIGhlnEsMN"
   },
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0.2,\n",
    "    mlp_dropout=0.2,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    x = layers.Dense(ff_dim)(x)\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "    \n",
    "    x = layers.Dense(1, activation=\"relu\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zogr-wp8E3wm"
   },
   "source": [
    "## Train and evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TruSmroqE13V",
    "outputId": "cd0c1410-246a-4997-aeb5-aa694ab82772"
   },
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=16,\n",
    "    num_heads=2,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=2,\n",
    "    mlp_units=[20,8],\n",
    "    mlp_dropout=0.2,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    batch_size=100,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "model.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "m-SDthZeWaVm",
    "outputId": "602c4de7-63c0-43ba-f1c9-4e585f298ca4"
   },
   "outputs": [],
   "source": [
    "metric = \"sparse_categorical_accuracy\"\n",
    "plt.figure()\n",
    "plt.plot(history.history[metric])\n",
    "plt.plot(history.history[\"val_\" + metric])\n",
    "plt.title(\"model \" + metric)\n",
    "plt.ylabel(metric, fontsize=\"large\")\n",
    "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
    "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"LIGO_transformer_based_classifier_v2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('LIGO_transformer_based_classifier_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "import os\n",
    "os.environ['PATH'] = '/opt/Xilinx/Vivado/2019.2/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#First, the baseline model\n",
    "hls_config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "\n",
    "# Set the precision and reuse factor for the full model\n",
    "hls_config['Model']['Precision'] = 'ap_fixed<18,8>'\n",
    "hls_config['Model']['ReuseFactor'] = 1\n",
    "hls_config['Model']['Strategy'] = 'Resource'\n",
    "\n",
    "for Layer in hls_config['LayerName'].keys():\n",
    "    hls_config['LayerName'][Layer]['Precision'] = 'ap_fixed<18,8>'\n",
    "    hls_config['LayerName'][Layer]['Strategy'] = 'Resource'\n",
    "    hls_config['LayerName'][Layer]['ReuseFactor'] = 1\n",
    "    hls_config['LayerName'][Layer]['weight'] = 'ap_fixed<18,8>'\n",
    "    hls_config['LayerName'][Layer]['scale'] = 'ap_fixed<18,8>'\n",
    "    hls_config['LayerName'][Layer]['bias'] = 'ap_fixed<18,8>'\n",
    "\n",
    "\n",
    "hls_config['LayerName']['layer_normalization']['table_range'] = 0.01\n",
    "hls_config['LayerName']['layer_normalization_1']['table_range'] = 4\n",
    "hls_config['LayerName']['layer_normalization_2']['table_range'] = 0.2\n",
    "hls_config['LayerName']['layer_normalization_3']['table_range'] = 0.5\n",
    "print(hls_config)\n",
    "\n",
    "\n",
    "cfg = hls4ml.converters.create_config(backend='Vivado')\n",
    "cfg['IOType']     = 'io_parallel' # Must set this if using CNNs!\n",
    "cfg['HLSConfig']  = hls_config\n",
    "cfg['KerasModel'] = model\n",
    "cfg['OutputDir']  = 'GW/GW_model_V2_reuse1'\n",
    "cfg['Part'] = 'xcvu13p-fhga2104-2L-e'\n",
    "  \n",
    "hls_model = hls4ml.converters.keras_to_hls(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_keras = model.predict(x_test[22:42,:,:])\n",
    "print(\"y_keras prediction is:\")\n",
    "print(y_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_hls = hls_model.predict(np.ascontiguousarray(x_test[22:42,:,:], dtype=np.float32))\n",
    "print(\"hls prediction is:\")\n",
    "print(y_hls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hls_model.build(csim=False, synth=True, vsynth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report('GW/GW_model_V2_reuse1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
